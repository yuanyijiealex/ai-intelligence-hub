# AI 情报日报 - 2025-10-16

> 时区：Asia/Shanghai | 抓取窗口：近 24 小时

共收录 42 条更新，按来源分组如下：

## Meta AI Blog (1)

- [[抓取失败] Meta AI Blog](https://ai.facebook.com/blog/feed/)  
  - 时间：2025-10-16 07:20 UTC
  - 摘要：错误：404 Client Error: Not Found for url: https://ai.meta.com/blog/feed/

## Stability AI Blog (1)

- [[抓取失败] Stability AI Blog](https://stability.ai/blog/rss.xml)  
  - 时间：2025-10-16 07:20 UTC
  - 摘要：错误：404 Client Error: Not Found for url: https://stability.ai/blog?format=rss

## arXiv cs.AI (20)

- [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12864v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This "rule-rigidity" is a significant barrier to building trustworthy autonomous agents. While prior…
- [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12979v1 Announce Type: new Abstract: Large language models (LLMs) augmented with multi-step reasoning and action generation abilities have shown promise in leveraging external tools to tackle complex tasks that require long-horizon planning. However, existing approaches either rely on implicit planning in the reasoning stage or introduce explicit planners without systematically address…
- [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12985v1 Announce Type: new Abstract: We present Sentinel, the first framework for formally evaluating the physical safety of Large Language Model(LLM-based) embodied agents across the semantic, plan, and trajectory levels. Unlike prior methods that rely on heuristic rules or subjective LLM judgments, Sentinel grounds practical safety requirements in formal temporal logic (TL) semantics…
- [From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model](https://arxiv.org/abs/2510.13002)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13002v1 Announce Type: new Abstract: Vehicle crashes involve complex interactions between road users, split-second decisions, and challenging environmental conditions. Among these, two-vehicle crashes are the most prevalent, accounting for approximately 70% of roadway crashes and posing a significant challenge to traffic safety. Identifying Driver Hazardous Action (DHA) is essential fo…
- [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13029v1 Announce Type: new Abstract: Traditional time series analysis has long relied on pattern recognition, trained on static and well-established benchmarks. However, in real-world settings -- where policies shift, human behavior adapts, and unexpected events unfold -- effective analysis must go beyond surface-level trends to uncover the actual forces driving them. The recent rise o…
- [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13036v1 Announce Type: new Abstract: Human-designed reward functions for reinforcement learning (RL) agents are frequently misaligned with the humans' true, unobservable objectives, and thus act only as proxies. Optimizing for a misspecified proxy reward function often induces reward hacking, resulting in a policy misaligned with the human's true objectives. An alternative is to perfor…
- [Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation](https://arxiv.org/abs/2510.13195)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13195v1 Announce Type: new Abstract: The advent of large language models (LLMs) has enabled agents to represent virtual humans in societal simulations, facilitating diverse interactions within complex social systems. However, existing LLM-based agents exhibit severe limitations in affective cognition: They fail to simulate the bounded rationality essential for bridging virtual and real…
- [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13214v1 Announce Type: new Abstract: Recent advances in Large Language Models (LLMs) demonstrate that chain-of-thought prompting and deep reasoning substantially enhance performance on complex tasks, and multi-agent systems can further improve accuracy by enabling model debates. However, applying deep reasoning to all problems is computationally expensive. To mitigate these costs, we p…
- [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13215v1 Announce Type: new Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning paths that align with individual goals. While large language models (LLMs) show potential in personalizing learning experiences, existing approaches often lack mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework for PLPP that integrates a reinforcem…
- [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13220v1 Announce Type: new Abstract: A fundamental limitation of current AI agents is their inability to learn complex skills on the fly at test time, often behaving like "clever but clueless interns" in novel environments. This severely limits their practical utility. To systematically measure and drive progress on this challenge, we first introduce the Jericho Test-Time Learning (J-T…
- [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13230v1 Announce Type: new Abstract: The driving environment perception has a vital role for autonomous driving and nowadays has been actively explored for its realization. The research community and relevant stakeholders necessitate the development of Deep Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles (AVs) for smart mobility. There is a need to develop …
- [SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2510.13262)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13262v1 Announce Type: new Abstract: Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for cooperative and competitive tasks such as autonomous driving and strategic gaming. However, models trained by MADRL are vulnerable to adversarial perturbations on states and actions. Therefore, it is essential to investigate the robustness of MADRL models from an attack perspect…
- [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13393v1 Announce Type: new Abstract: Rationalization, a data-centric framework, aims to build self-explanatory models to explain the prediction outcome by generating a subset of human-intelligible pieces of the input data. It involves a cooperative game model where a generator generates the most human-intelligible parts of the input (i.e., rationales), followed by a predictor that make…
- [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13417v1 Announce Type: new Abstract: How does a cause lead to an effect, and which intermediate causal steps explain their connection? This work scrutinizes the mechanistic causal reasoning capabilities of large language models (LLMs) to answer these questions through the task of implicit causal chain discovery. In a diagnostic evaluation framework, we instruct nine LLMs to generate al…
- [Mobile Coverage Analysis using Crowdsourced Data](https://arxiv.org/abs/2510.13459)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13459v1 Announce Type: new Abstract: Effective assessment of mobile network coverage and the precise identification of service weak spots are paramount for network operators striving to enhance user Quality of Experience (QoE). This paper presents a novel framework for mobile coverage and weak spot analysis utilising crowdsourced QoE data. The core of our methodology involves coverage …
- [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13501v1 Announce Type: new Abstract: Reward models can significantly enhance the reasoning capabilities of large language models (LLMs), but they typically require extensive curated data and costly training. To mitigate these challenges, training-free approaches such as LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate responses, achieving promising results.…
- [A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain](https://arxiv.org/abs/2510.13524)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13524v1 Announce Type: new Abstract: As Generative Artificial Intelligence is adopted across the financial services industry, a significant barrier to adoption and usage is measuring model performance. Historical machine learning metrics can oftentimes fail to generalize to GenAI workloads and are often supplemented using Subject Matter Expert (SME) Evaluation. Even in this combination…
- [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13551v1 Announce Type: new Abstract: As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators…
- [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13691v1 Announce Type: new Abstract: Logic-based models can be used to build verification tools for machine learning classifiers employed in the legal field. ML classifiers predict the outcomes of new cases based on previous ones, thereby performing a form of case-based reasoning (CBR). In this paper, we introduce a modal logic of classifiers designed to formally capture legal CBR. We …
- [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13709v1 Announce Type: new Abstract: Assistive agents should not only take actions on behalf of a human, but also step out of the way and cede control when there are important decisions to be made. However, current methods for building assistive agents, whether via mimicking expert humans or via RL finetuning on an inferred reward, often encourage agents to complete tasks on their own …

## arXiv cs.CL (20)

- [Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning](https://arxiv.org/abs/2510.12807)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12807v1 Announce Type: new Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous languages; however, their effectiveness in low-resource languages like Persian requires thorough investigation. This paper presents a comprehensive benchmark of several open-source LLMs for Persian Natural Language Processing (NLP) tasks, utilizing both zero-shot …
- [Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study](https://arxiv.org/abs/2510.12813)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12813v1 Announce Type: new Abstract: Electronic health records contain inconsistently structured or free-text data, requiring efficient preprocessing to enable predictive health care models. Although artificial intelligence-driven natural language processing tools show promise for automating diagnosis classification, their comparative performance and clinical reliability require system…
- [From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP](https://arxiv.org/abs/2510.12817)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12817v1 Announce Type: new Abstract: Human Label Variation (HLV) refers to legitimate disagreement in annotation that reflects the genuine diversity of human perspectives rather than mere error. For decades, HLV in NLP was dismissed as noise to be discarded, and only slowly over the last decade has it been reframed as a signal for improving model robustness. With the rise of large lang…
- [MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning](https://arxiv.org/abs/2510.12818)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12818v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly deployed in clinical decision support, yet subtle demographic cues can influence their reasoning. Prior work has documented disparities in outputs across patient groups, but little is known about how internal reasoning shifts under controlled demographic changes. We introduce MEDEQUALQA, a counterfactual…
- [Classifier-Augmented Generation for Structured Workflow Prediction](https://arxiv.org/abs/2510.12825)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12825v1 Announce Type: new Abstract: ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to visually assemble complex data workflows, but configuring stages and their properties remains time consuming and requires deep tool knowledge. We propose a system that translates natural language descriptions into executable workflows, automatically predicting both the structu…
- [Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/abs/2510.12826)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12826v1 Announce Type: new Abstract: As large language model (LLM) agents are deployed autonomously in diverse contexts, evaluating their capacity for strategic deception becomes crucial. While recent research has examined how AI systems scheme against human developers, LLM-to-LLM scheming remains underexplored. We investigate the scheming ability and propensity of frontier LLM agents …
- [Mathematics with large language models as provers and verifiers](https://arxiv.org/abs/2510.12829)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12829v1 Announce Type: new Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of large language models started reporting interesting success stories, mostly to do with difficult exercises (such as problems from the International Mathematical Olympiad), but also with conjectures [Feldman & Karbasi, arXiv:2509.18383v1] formulated for the purpose of verif…
- [MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training](https://arxiv.org/abs/2510.12831)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12831v1 Announce Type: new Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances into executable SQL while preserving dialogue coherence and grounding to the target schema. However, most existing systems only regard this task as a simple text translation task and follow a short-horizon paradigm, generating a query per turn without execution, explicit ver…
- [Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study](https://arxiv.org/abs/2510.12835)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12835v1 Announce Type: new Abstract: This study investigates how existing annotation guidelines can be repurposed to instruct large language model (LLM) annotators for text annotation tasks. Traditional guidelines are written for human annotators who internalize training, while LLMs require explicit, structured instructions. We propose a moderation-oriented guideline repurposing method…
- [A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning](https://arxiv.org/abs/2510.12838)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12838v1 Announce Type: new Abstract: Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to m…
- [FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs](https://arxiv.org/abs/2510.12839)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12839v1 Announce Type: new Abstract: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to accuracy issues and costly human assessment. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to complex pipeline components …
- [VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages](https://arxiv.org/abs/2510.12845)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12845v1 Announce Type: new Abstract: Vision Language Models (VLMs) are pivotal for advancing perception in intelligent agents. Yet, evaluation of VLMs remains limited to predominantly English-centric benchmarks in which the image-text pairs comprise short texts. To evaluate VLM fine-grained abilities, in four languages under long-text settings, we introduce a novel multilingual benchma…
- [Efficient Adaptive Transformer: An Empirical Study and Reproducible Framework](https://arxiv.org/abs/2510.12856)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12856v1 Announce Type: new Abstract: The Efficient Adaptive Transformer (EAT) framework unifies three adaptive efficiency techniques - progressive token pruning, sparse attention, and dynamic early exiting - into a single, reproducible architecture for input-adaptive inference. EAT provides an open-source benchmarking pipeline that automates data processing, timing, and ablation across…
- [A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation](https://arxiv.org/abs/2510.12858)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12858v1 Announce Type: new Abstract: The sacred practice of Quranic recitation (Tajweed), governed by precise phonetic, prosodic, and theological rules, faces significant pedagogical challenges in the modern era. While digital technologies promise unprecedented access to education, automated tools for recitation evaluation have failed to achieve widespread adoption or pedagogical effic…
- [EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus](https://arxiv.org/abs/2510.12899)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12899v1 Announce Type: new Abstract: Recently, several multi-turn dialogue benchmarks have been proposed to evaluate the conversational abilities of large language models (LLMs). As LLMs are increasingly recognized as a key technology for advancing intelligent education, owing to their ability to deeply understand instructional contexts and provide personalized guidance, the constructi…
- [Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering](https://arxiv.org/abs/2510.12925)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12925v1 Announce Type: new Abstract: Large Language Models (LLMs) should answer factual questions truthfully, grounded in objective knowledge, regardless of user context such as self-disclosed personal information, or system personalization. In this paper, we present the first systematic evaluation of LLM robustness to inquiry personas, i.e. user profiles that convey attributes like id…
- [The Curious Case of Curiosity across Human Cultures and LLMs](https://arxiv.org/abs/2510.12943)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12943v1 Announce Type: new Abstract: Recent advances in Large Language Models (LLMs) have expanded their role in human interaction, yet curiosity -- a central driver of inquiry -- remains underexplored in these systems, particularly across cultural contexts. In this work, we investigate cultural variation in curiosity using Yahoo! Answers, a real-world multi-country dataset spanning di…
- [3-Model Speculative Decoding](https://arxiv.org/abs/2510.12966)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12966v1 Announce Type: new Abstract: Speculative Decoding (SD) accelerates inference in large language models by using a smaller draft model to propose tokens, which are then verified by a larger target model. However, the throughput gains of SD are fundamentally limited by a trade-off between draft model size and token acceptance: smaller draft models generate tokens more quickly but …
- [A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation](https://arxiv.org/abs/2510.12993)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.12993v1 Announce Type: new Abstract: The human-like proficiency of Large Language Models (LLMs) has brought concerns about their potential misuse for generating persuasive and personalised disinformation at scale. While prior work has demonstrated that LLMs can generate disinformation, specific questions around persuasiveness and personalisation (generation of disinformation tailored t…
- [OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.13003)  
  - 时间：2025-10-16 04:00 UTC
  - 摘要：arXiv:2510.13003v1 Announce Type: new Abstract: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language models but suffers from catastrophic forgetting when learned updates interfere with the dominant singular directions that encode essential pre-trained knowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically grounded approach that prevents this interference …
